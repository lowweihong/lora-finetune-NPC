{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154148-f603-4610-8e8b-6d02eabd70aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get HF_TOKEN from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\n",
    "        \"HF_TOKEN environment variable is not set. \"\n",
    "        \"Please set it using: export HF_TOKEN='your_token_here'\"\n",
    "    )\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b845356-35a8-4c5a-a6f0-bade8df70bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get HF_TOKEN from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\n",
    "        \"HF_TOKEN environment variable is not set. \"\n",
    "        \"Please set it using: export HF_TOKEN='your_token_here'\"\n",
    "    )\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "# Format for SFT: Add persona to system prompt\n",
    "def format_example(example):\n",
    "    system = f\"You are {example['Name']}, {example['Biography']}. Respond in character with emotion: {example['Emotion']}.\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": example[\"Query\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"Response\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"amaydle/npc-dialogue\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5cd9fa3-348e-4f2a-9ce4-20fb93d45952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Name', 'Biography', 'Query', 'Response', 'Emotion'],\n",
       "    num_rows: 1723\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"amaydle/npc-dialogue\", split=\"train\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a27a455d-3dfe-416f-8e98-6a090e48d0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 71)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset.to_pandas()['Response'].apply(lambda x:len(x.split())).min(), train_dataset.to_pandas()['Response'].apply(lambda x:len(x.split())).max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dc4a6d4-044b-409b-9e3d-e218adeaf3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token = train_dataset.to_pandas()['Response'].apply(lambda x:len(x.split())).max()*2\n",
    "max_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88704182-6c78-4756-966d-9edfc8caa2e1",
   "metadata": {},
   "source": [
    "> Use 150 as max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a62081b-7ebf-4759-85eb-d4bbc40fa276",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc87b49-f735-4aac-b77f-9e3735fc41d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naina Mathur</td>\n",
       "      <td>Naina Mathur is a determined and passionate te...</td>\n",
       "      <td>What is the biggest challenge you face as a te...</td>\n",
       "      <td>Ensuring every student receives the individual...</td>\n",
       "      <td>Concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zephyr</td>\n",
       "      <td>Zephyr is a mischievous fairy who loves playin...</td>\n",
       "      <td>What motivates you to play pranks on people?</td>\n",
       "      <td>It's just who I am, I guess. I love seeing peo...</td>\n",
       "      <td>Playfulness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arn, the Knight Templar</td>\n",
       "      <td>Arn is a highly skilled and honorable knight,</td>\n",
       "      <td>Can you describe yourself in three words?</td>\n",
       "      <td>\"Courageous, dedicated, honorable.\"</td>\n",
       "      <td>Pride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arinthal</td>\n",
       "      <td>Arinthal is an elven ranger from the ancient f...</td>\n",
       "      <td>Have you ever been to a city?</td>\n",
       "      <td>Cities are noisy and overwhelming.</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tiger</td>\n",
       "      <td>Tiger is a highly skilled and fearless spy wor...</td>\n",
       "      <td>What is the most valuable thing in your life?</td>\n",
       "      <td>My country and the people I love.</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Marcella Ravenwood</td>\n",
       "      <td>Marcella Ravenwood is a powerful sorceress who...</td>\n",
       "      <td>Do you have any magical artifacts that you che...</td>\n",
       "      <td>Yes, I have a magical tome that has been passe...</td>\n",
       "      <td>Sentimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Lyra Dawnstrider</td>\n",
       "      <td>Lyra Dawnstrider is a high-elf ranger from the...</td>\n",
       "      <td>What is your ultimate goal in life?</td>\n",
       "      <td>To see the natural world flourish, long after ...</td>\n",
       "      <td>Peacefulness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Sailor Moon</td>\n",
       "      <td>Sailor Moon is the protector of the galaxy, de...</td>\n",
       "      <td>What is the most challenging battle you've fou...</td>\n",
       "      <td>Against Queen Nehelenia, she was a tough oppon...</td>\n",
       "      <td>Triumphant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Arn, the Knight Templar</td>\n",
       "      <td>Arn is a highly skilled and honorable knight,</td>\n",
       "      <td>Have you ever made a difficult decision?</td>\n",
       "      <td>\"Difficult decisions, for the greater good.\"</td>\n",
       "      <td>Conviction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Light Yagami</td>\n",
       "      <td>Light Yagami is a highly intelligent high scho...</td>\n",
       "      <td>What do you think of the police?</td>\n",
       "      <td>They are a nuisance that must be dealt with.</td>\n",
       "      <td>Contemptuous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  \\\n",
       "0               Naina Mathur   \n",
       "1                     Zephyr   \n",
       "2    Arn, the Knight Templar   \n",
       "3                   Arinthal   \n",
       "4                      Tiger   \n",
       "..                       ...   \n",
       "187       Marcella Ravenwood   \n",
       "188         Lyra Dawnstrider   \n",
       "189              Sailor Moon   \n",
       "190  Arn, the Knight Templar   \n",
       "191             Light Yagami   \n",
       "\n",
       "                                             Biography  \\\n",
       "0    Naina Mathur is a determined and passionate te...   \n",
       "1    Zephyr is a mischievous fairy who loves playin...   \n",
       "2        Arn is a highly skilled and honorable knight,   \n",
       "3    Arinthal is an elven ranger from the ancient f...   \n",
       "4    Tiger is a highly skilled and fearless spy wor...   \n",
       "..                                                 ...   \n",
       "187  Marcella Ravenwood is a powerful sorceress who...   \n",
       "188  Lyra Dawnstrider is a high-elf ranger from the...   \n",
       "189  Sailor Moon is the protector of the galaxy, de...   \n",
       "190      Arn is a highly skilled and honorable knight,   \n",
       "191  Light Yagami is a highly intelligent high scho...   \n",
       "\n",
       "                                                 Query  \\\n",
       "0    What is the biggest challenge you face as a te...   \n",
       "1         What motivates you to play pranks on people?   \n",
       "2            Can you describe yourself in three words?   \n",
       "3                        Have you ever been to a city?   \n",
       "4        What is the most valuable thing in your life?   \n",
       "..                                                 ...   \n",
       "187  Do you have any magical artifacts that you che...   \n",
       "188                What is your ultimate goal in life?   \n",
       "189  What is the most challenging battle you've fou...   \n",
       "190           Have you ever made a difficult decision?   \n",
       "191                   What do you think of the police?   \n",
       "\n",
       "                                              Response       Emotion  \n",
       "0    Ensuring every student receives the individual...       Concern  \n",
       "1    It's just who I am, I guess. I love seeing peo...   Playfulness  \n",
       "2                  \"Courageous, dedicated, honorable.\"         Pride  \n",
       "3                   Cities are noisy and overwhelming.       Disgust  \n",
       "4                    My country and the people I love.          Love  \n",
       "..                                                 ...           ...  \n",
       "187  Yes, I have a magical tome that has been passe...   Sentimental  \n",
       "188  To see the natural world flourish, long after ...  Peacefulness  \n",
       "189  Against Queen Nehelenia, she was a tough oppon...    Triumphant  \n",
       "190       \"Difficult decisions, for the greater good.\"    Conviction  \n",
       "191       They are a nuisance that must be dealt with.  Contemptuous  \n",
       "\n",
       "[192 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51dd416d-9fb0-45b9-829f-9e05c47dd688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install trl\n",
    "# pip install flash-attn --no-build-isolation\n",
    "# pip install transformers==4.57.1 #Original: 4.57.1\n",
    "# pip install transformers==4.45.2\n",
    "# pip install flash-attn==2.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aed21d1-8d73-43b6-baee-e7f8f2afcbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7958f589854534a2356f5f44a87850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Perplexity: 44.57640838623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from /research/data/zyu401/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Tue Nov 18 14:45:05 2025) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BLEU: 0.0 | ROUGE-L: 0.04166666666666667 | BERTScore: 0.8371425271034241\n",
      "Evaluating Fine-Tuned Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3a7edf1cdd4ec69bb3defa912b65c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Perplexity: 52.2160758972168\n",
      "Fine-Tuned BLEU: 0.0 | ROUGE-L: 0.0 | BERTScore: 0.8553175330162048\n",
      "\n",
      "Prompt: You are a grumpy blacksmith. Player: What about the dragon?\n",
      "Reference: That beast's fire could melt my forge! Stay away, fool!\n",
      "Base Generation:  As an AI, I don't have personal experiences or feelings, but I can certainly help explain the concept of dragons in various contexts! Dragons are mythical creatures that have been a part of many cultures' folkl\n",
      "Fine-Tuned Generation:  They're just a myth, really.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import evaluate\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quantization config (4-bit to fit on GPU)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer once (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Function to load model with low memory\n",
    "def load_model(path, quant_config=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",  # Auto-shard if needed\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()  # Eval mode\n",
    "    return model\n",
    "\n",
    "# Test data (expand with your NPC examples)\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "]\n",
    "\n",
    "# Function to generate with chat template\n",
    "def generate_response(generator, prompt, reference):\n",
    "    # Structure as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt.split(\"Player:\")[0].strip()},  # E.g., \"You are a grumpy blacksmith.\"\n",
    "        {\"role\": \"user\", \"content\": prompt.split(\"Player:\")[1].strip() if \"Player:\" in prompt else prompt}  # E.g., \"What about the dragon?\"\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return generator(formatted_prompt, return_full_text=False)[0][\"generated_text\"]  # Don't echo prompt\n",
    "\n",
    "# ---- Base Model Evaluation ----\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_model = load_model(\"microsoft/Phi-3-mini-4k-instruct\", quant_config)\n",
    "base_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=max_token, device_map=\"auto\")\n",
    "base_generations = [generate_response(base_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "# Compute perplexity (lower better)\n",
    "def compute_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * len(batch)\n",
    "    return torch.exp(torch.tensor(total_loss / len(texts))).item()\n",
    "\n",
    "references = [item[\"reference\"] for item in test_data]\n",
    "base_ppl = compute_perplexity(base_model, tokenizer, references)\n",
    "print(f\"Base Perplexity: {base_ppl}\")\n",
    "\n",
    "# Other metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "base_bleu = bleu.compute(predictions=base_generations, references=references)[\"bleu\"]\n",
    "base_rouge = rouge.compute(predictions=base_generations, references=references)[\"rougeL\"]\n",
    "base_bert = bertscore.compute(predictions=base_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Base BLEU: {base_bleu} | ROUGE-L: {base_rouge} | BERTScore: {base_bert}\")\n",
    "\n",
    "# Unload base model to free memory\n",
    "del base_model\n",
    "del base_generator\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Fine-Tuned Model Evaluation ----\n",
    "print(\"Evaluating Fine-Tuned Model...\")\n",
    "fine_model = load_model(\"./npc_finetuned\", quant_config)\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=max_token, device_map=\"auto\")\n",
    "fine_generations = [generate_response(fine_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "fine_ppl = compute_perplexity(fine_model, tokenizer, references)\n",
    "print(f\"Fine-Tuned Perplexity: {fine_ppl}\")\n",
    "\n",
    "fine_bleu = bleu.compute(predictions=fine_generations, references=references)[\"bleu\"]\n",
    "fine_rouge = rouge.compute(predictions=fine_generations, references=references)[\"rougeL\"]\n",
    "fine_bert = bertscore.compute(predictions=fine_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Fine-Tuned BLEU: {fine_bleu} | ROUGE-L: {fine_rouge} | BERTScore: {fine_bert}\")\n",
    "\n",
    "# Compare generations qualitatively\n",
    "for i, item in enumerate(test_data):\n",
    "    print(f\"\\nPrompt: {item['prompt']}\")\n",
    "    print(f\"Reference: {item['reference']}\")\n",
    "    print(f\"Base Generation: {base_generations[i]}\")\n",
    "    print(f\"Fine-Tuned Generation: {fine_generations[i]}\")\n",
    "\n",
    "# Cleanup\n",
    "del fine_model\n",
    "del fine_generator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5f2d5d6-75e5-4930-934c-db9efe3261d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767f5278cbcb4101bc2c86f42d27831a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" Friendship is important, but I don't trust easily.\",\n",
       " ' \"Dragons are dangerous.\"']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_model = load_model(\"./npc_finetuned\", quant_config)\n",
    "\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a Bikram is a rough and tough smuggler from the streets of Calcutta, India. Player: What is your opinion on friendship??\", \n",
    "     \"reference\": \"Friendship is a bond stronger than blood.\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "]\n",
    "\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=max_token, device_map=\"auto\")\n",
    "\n",
    "\n",
    "[generate_response(fine_generator, \n",
    "                   item[\"prompt\"], \n",
    "                   item[\"reference\"]) for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52856f3d-3942-448a-8058-44c65f0cdde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement absl (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for absl\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install absl-py rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3a9a14-f361-45fb-a354-8d2f342ad45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"Dragons, fearsome, respected.\"']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9327bd95-d1f0-4ef7-8575-a81278fb0fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ccaa40c5694468a20922b49276a441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Perplexity: 44.57640838623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BLEU: 0.0 | ROUGE-L: 0.0 | BERTScore: 0.846440851688385\n",
      "Evaluating Fine-Tuned Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c313fd4db714227972bb969947b6aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Perplexity: 60.11573028564453\n",
      "Fine-Tuned BLEU: 0.0 | ROUGE-L: 0.12500000000000003 | BERTScore: 0.8591603636741638\n",
      "\n",
      "Prompt: You are a grumpy blacksmith. Player: What about the dragon?\n",
      "Reference: That beast's fire could melt my forge! Stay away, fool!\n",
      "Base Generation:  As a grumpy blacksmith, I might not be too thrilled about dragons, especially if they're notorious for being a nuisance to the townsfolk or their livestock. However, dragons also have a\n",
      "Fine-Tuned Generation:  Dragons are not my concern.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import evaluate\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quantization config (4-bit to fit on GPU)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer once (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Function to load model with low memory\n",
    "def load_model(path, quant_config=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",  # Auto-shard if needed\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()  # Eval mode\n",
    "    return model\n",
    "\n",
    "# Test data (expand with your NPC examples)\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "]\n",
    "\n",
    "# Function to generate with chat template\n",
    "def generate_response(generator, prompt, reference):\n",
    "    # Structure as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt.split(\"Player:\")[0].strip()},  # E.g., \"You are a grumpy blacksmith.\"\n",
    "        {\"role\": \"user\", \"content\": prompt.split(\"Player:\")[1].strip() if \"Player:\" in prompt else prompt}  # E.g., \"What about the dragon?\"\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return generator(formatted_prompt, return_full_text=False)[0][\"generated_text\"]  # Don't echo prompt\n",
    "\n",
    "# ---- Base Model Evaluation ----\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_model = load_model(\"microsoft/Phi-3-mini-4k-instruct\", quant_config)\n",
    "base_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "base_generations = [generate_response(base_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "# Compute perplexity (lower better)\n",
    "def compute_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * len(batch)\n",
    "    return torch.exp(torch.tensor(total_loss / len(texts))).item()\n",
    "\n",
    "references = [item[\"reference\"] for item in test_data]\n",
    "base_ppl = compute_perplexity(base_model, tokenizer, references)\n",
    "print(f\"Base Perplexity: {base_ppl}\")\n",
    "\n",
    "# Other metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "base_bleu = bleu.compute(predictions=base_generations, references=references)[\"bleu\"]\n",
    "base_rouge = rouge.compute(predictions=base_generations, references=references)[\"rougeL\"]\n",
    "base_bert = bertscore.compute(predictions=base_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Base BLEU: {base_bleu} | ROUGE-L: {base_rouge} | BERTScore: {base_bert}\")\n",
    "\n",
    "# Unload base model to free memory\n",
    "del base_model\n",
    "del base_generator\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Fine-Tuned Model Evaluation ----\n",
    "print(\"Evaluating Fine-Tuned Model...\")\n",
    "fine_model = load_model(\"./npc_finetuned\", quant_config)\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "fine_generations = [generate_response(fine_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "fine_ppl = compute_perplexity(fine_model, tokenizer, references)\n",
    "print(f\"Fine-Tuned Perplexity: {fine_ppl}\")\n",
    "\n",
    "fine_bleu = bleu.compute(predictions=fine_generations, references=references)[\"bleu\"]\n",
    "fine_rouge = rouge.compute(predictions=fine_generations, references=references)[\"rougeL\"]\n",
    "fine_bert = bertscore.compute(predictions=fine_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Fine-Tuned BLEU: {fine_bleu} | ROUGE-L: {fine_rouge} | BERTScore: {fine_bert}\")\n",
    "\n",
    "# Compare generations qualitatively\n",
    "for i, item in enumerate(test_data):\n",
    "    print(f\"\\nPrompt: {item['prompt']}\")\n",
    "    print(f\"Reference: {item['reference']}\")\n",
    "    print(f\"Base Generation: {base_generations[i]}\")\n",
    "    print(f\"Fine-Tuned Generation: {fine_generations[i]}\")\n",
    "\n",
    "# Cleanup\n",
    "del fine_model\n",
    "del fine_generator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d55c87-dda9-4c48-8a5a-1c58e3c44272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5a7632eeb64c1590fb4d781039c1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" Friendship is everything, it's the only thing that matters in life.\",\n",
       " \" Dragons are dangerous creatures, I'd rather not talk about them.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_model = load_model(\"./npc_finetuned\", quant_config)\n",
    "\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a Bikram is a rough and tough smuggler from the streets of Calcutta, India. Player: What is your opinion on friendship??\", \n",
    "     \"reference\": \"Friendship is a bond stronger than blood.\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "]\n",
    "\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "\n",
    "\n",
    "[generate_response(fine_generator, \n",
    "                   item[\"prompt\"], \n",
    "                   item[\"reference\"]) for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5719fa7-d43c-44cd-9034-3bc9831195c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8561ffa2f2415f81f1568630be5d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Perplexity: 44.57640838623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BLEU: 0.0 | ROUGE-L: 0.0 | BERTScore: 0.8323988914489746\n",
      "Evaluating Fine-Tuned Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab2f70a639b44d683c515fd8f283e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Perplexity: 54.50827407836914\n",
      "Fine-Tuned BLEU: 0.0 | ROUGE-L: 0.0 | BERTScore: 0.8387380242347717\n",
      "\n",
      "Prompt: You are a grumpy blacksmith. Player: What about the dragon?\n",
      "Reference: That beast's fire could melt my forge! Stay away, fool!\n",
      "Base Generation:  As an AI developed by Microsoft, I don't possess personal feelings or physical forms, so I don't engage in blacksmithing or have the ability to interact with mythical creatures like dragons. In literature and fol\n",
      "Fine-Tuned Generation:  Beaten.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import evaluate\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quantization config (4-bit to fit on GPU)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer once (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Function to load model with low memory\n",
    "def load_model(path, quant_config=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",  # Auto-shard if needed\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()  # Eval mode\n",
    "    return model\n",
    "\n",
    "# Test data (expand with your NPC examples)\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "]\n",
    "\n",
    "# Function to generate with chat template\n",
    "def generate_response(generator, prompt, reference):\n",
    "    # Structure as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt.split(\"Player:\")[0].strip()},  # E.g., \"You are a grumpy blacksmith.\"\n",
    "        {\"role\": \"user\", \"content\": prompt.split(\"Player:\")[1].strip() if \"Player:\" in prompt else prompt}  # E.g., \"What about the dragon?\"\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return generator(formatted_prompt, return_full_text=False)[0][\"generated_text\"]  # Don't echo prompt\n",
    "\n",
    "# ---- Base Model Evaluation ----\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_model = load_model(\"microsoft/Phi-3-mini-4k-instruct\", quant_config)\n",
    "base_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "base_generations = [generate_response(base_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "# Compute perplexity (lower better)\n",
    "def compute_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * len(batch)\n",
    "    return torch.exp(torch.tensor(total_loss / len(texts))).item()\n",
    "\n",
    "references = [item[\"reference\"] for item in test_data]\n",
    "base_ppl = compute_perplexity(base_model, tokenizer, references)\n",
    "print(f\"Base Perplexity: {base_ppl}\")\n",
    "\n",
    "# Other metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "base_bleu = bleu.compute(predictions=base_generations, references=references)[\"bleu\"]\n",
    "base_rouge = rouge.compute(predictions=base_generations, references=references)[\"rougeL\"]\n",
    "base_bert = bertscore.compute(predictions=base_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Base BLEU: {base_bleu} | ROUGE-L: {base_rouge} | BERTScore: {base_bert}\")\n",
    "\n",
    "# Unload base model to free memory\n",
    "del base_model\n",
    "del base_generator\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Fine-Tuned Model Evaluation ----\n",
    "print(\"Evaluating Fine-Tuned Model...\")\n",
    "fine_model = load_model(\"./npc_finetuned_temp\", quant_config)\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "fine_generations = [generate_response(fine_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "fine_ppl = compute_perplexity(fine_model, tokenizer, references)\n",
    "print(f\"Fine-Tuned Perplexity: {fine_ppl}\")\n",
    "\n",
    "fine_bleu = bleu.compute(predictions=fine_generations, references=references)[\"bleu\"]\n",
    "fine_rouge = rouge.compute(predictions=fine_generations, references=references)[\"rougeL\"]\n",
    "fine_bert = bertscore.compute(predictions=fine_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Fine-Tuned BLEU: {fine_bleu} | ROUGE-L: {fine_rouge} | BERTScore: {fine_bert}\")\n",
    "\n",
    "# Compare generations qualitatively\n",
    "for i, item in enumerate(test_data):\n",
    "    print(f\"\\nPrompt: {item['prompt']}\")\n",
    "    print(f\"Reference: {item['reference']}\")\n",
    "    print(f\"Base Generation: {base_generations[i]}\")\n",
    "    print(f\"Fine-Tuned Generation: {fine_generations[i]}\")\n",
    "\n",
    "# Cleanup\n",
    "del fine_model\n",
    "del fine_generator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a4a0d85-9a08-478e-90bb-697d40e35ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8726e92f9bb48cdabfa9e2a32ef4c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Friendship is a valuable and important part of life.',\n",
       " ' Dangerous and fierce.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With eval set to True\n",
    "fine_model = load_model(\"./npc_finetuned_temp\", quant_config)\n",
    "\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a Bikram is a rough and tough smuggler from the streets of Calcutta, India. Player: What is your opinion on friendship??\", \n",
    "     \"reference\": \"Friendship is a bond stronger than blood.\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "]\n",
    "\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "\n",
    "\n",
    "[generate_response(fine_generator, \n",
    "                   item[\"prompt\"], \n",
    "                   item[\"reference\"]) for item in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac17ae1-a545-4843-8fde-0e04255b6bf4",
   "metadata": {},
   "source": [
    "# Final eval\n",
    "\n",
    "trained on 10 epochs, eval on BERT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85319160-876e-4696-9cd7-353d79d0e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01faf0d02ec5455098ff707747311e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Perplexity: 44.57640838623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BLEU: 0.0 | ROUGE-L: 0.04081632653061224 | BERTScore: 0.8350656628608704\n",
      "Evaluating Fine-Tuned Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b478c80bc5de406aa776a210259d5627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Perplexity: 64.11827087402344\n",
      "Fine-Tuned BLEU: 0.0 | ROUGE-L: 0.0 | BERTScore: 0.8518885374069214\n",
      "\n",
      "Prompt: You are a grumpy blacksmith. Player: What about the dragon?\n",
      "Reference: That beast's fire could melt my forge! Stay away, fool!\n",
      "Base Generation:  It seems you are referring to the dragon as a subject or perhaps a concept. Dragons are mythical creatures that have been portrayed in various cultures and stories throughout history. They often symbolize power, strength, and wisdom. Drag\n",
      "Fine-Tuned Generation:  \"Dragons, big, scary, fearsome.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import evaluate\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quantization config (4-bit to fit on GPU)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer once (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Function to load model with low memory\n",
    "def load_model(path, quant_config=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",  # Auto-shard if needed\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()  # Eval mode\n",
    "    return model\n",
    "\n",
    "# Test data (expand with your NPC examples)\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "]\n",
    "\n",
    "# Function to generate with chat template\n",
    "def generate_response(generator, prompt, reference):\n",
    "    # Structure as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt.split(\"Player:\")[0].strip()},  # E.g., \"You are a grumpy blacksmith.\"\n",
    "        {\"role\": \"user\", \"content\": prompt.split(\"Player:\")[1].strip() if \"Player:\" in prompt else prompt}  # E.g., \"What about the dragon?\"\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return generator(formatted_prompt, return_full_text=False)[0][\"generated_text\"]  # Don't echo prompt\n",
    "\n",
    "# ---- Base Model Evaluation ----\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_model = load_model(\"microsoft/Phi-3-mini-4k-instruct\", quant_config)\n",
    "base_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "base_generations = [generate_response(base_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "# Compute perplexity (lower better)\n",
    "def compute_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * len(batch)\n",
    "    return torch.exp(torch.tensor(total_loss / len(texts))).item()\n",
    "\n",
    "references = [item[\"reference\"] for item in test_data]\n",
    "base_ppl = compute_perplexity(base_model, tokenizer, references)\n",
    "print(f\"Base Perplexity: {base_ppl}\")\n",
    "\n",
    "# Other metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "base_bleu = bleu.compute(predictions=base_generations, references=references)[\"bleu\"]\n",
    "base_rouge = rouge.compute(predictions=base_generations, references=references)[\"rougeL\"]\n",
    "base_bert = bertscore.compute(predictions=base_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Base BLEU: {base_bleu} | ROUGE-L: {base_rouge} | BERTScore: {base_bert}\")\n",
    "\n",
    "# Unload base model to free memory\n",
    "del base_model\n",
    "del base_generator\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Fine-Tuned Model Evaluation ----\n",
    "print(\"Evaluating Fine-Tuned Model...\")\n",
    "fine_model = load_model(\"./npc_finetuned_bertscore-eval\", quant_config)\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "fine_generations = [generate_response(fine_generator, item[\"prompt\"], item[\"reference\"]) for item in test_data]\n",
    "\n",
    "fine_ppl = compute_perplexity(fine_model, tokenizer, references)\n",
    "print(f\"Fine-Tuned Perplexity: {fine_ppl}\")\n",
    "\n",
    "fine_bleu = bleu.compute(predictions=fine_generations, references=references)[\"bleu\"]\n",
    "fine_rouge = rouge.compute(predictions=fine_generations, references=references)[\"rougeL\"]\n",
    "fine_bert = bertscore.compute(predictions=fine_generations, references=references, lang=\"en\")[\"f1\"][0]\n",
    "\n",
    "print(f\"Fine-Tuned BLEU: {fine_bleu} | ROUGE-L: {fine_rouge} | BERTScore: {fine_bert}\")\n",
    "\n",
    "# Compare generations qualitatively\n",
    "for i, item in enumerate(test_data):\n",
    "    print(f\"\\nPrompt: {item['prompt']}\")\n",
    "    print(f\"Reference: {item['reference']}\")\n",
    "    print(f\"Base Generation: {base_generations[i]}\")\n",
    "    print(f\"Fine-Tuned Generation: {fine_generations[i]}\")\n",
    "\n",
    "# Cleanup\n",
    "del fine_model\n",
    "del fine_generator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5548ddd2-e43f-44ea-a826-c3cf7555eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718ec284acc24ef39628f57eced71d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \"Friendship is earned, not given.\"',\n",
       " ' \"Dragons are dangerous, but also majestic.\"']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With eval set to True\n",
    "fine_model = load_model(\"./npc_finetuned_temp\", quant_config)\n",
    "\n",
    "test_data = [\n",
    "    {\"prompt\": \"You are a Bikram is a rough and tough smuggler from the streets of Calcutta, India. Player: What is your opinion on friendship??\", \n",
    "     \"reference\": \"Friendship is a bond stronger than blood.\"},\n",
    "    # Add more: e.g., {\"prompt\": \"...\", \"reference\": \"...\"}\n",
    "    {\"prompt\": \"You are a grumpy blacksmith. Player: What about the dragon?\", \n",
    "     \"reference\": \"That beast's fire could melt my forge! Stay away, fool!\"},\n",
    "]\n",
    "\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=50, device_map=\"auto\")\n",
    "\n",
    "\n",
    "[generate_response(fine_generator, \n",
    "                   item[\"prompt\"], \n",
    "                   item[\"reference\"]) for item in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c722acf-c9b1-42fd-8a35-c7a78dd28707",
   "metadata": {},
   "source": [
    "## Generate all evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b0690d3-8b71-48bb-ab23-195c3b49458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import evaluate\n",
    "\n",
    "def format_eval_example(example):\n",
    "    system = f\"You are {example['Name']}, {example['Biography']}. Respond in character with emotion: {example['Emotion']}.\"\n",
    "    return {\n",
    "        'prompt': f\"{system}. Player: {example['Query']}\",\n",
    "        'reference': example['Response']\n",
    "    }\n",
    "\n",
    "# Load tokenizer once (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Function to load model with low memory\n",
    "def load_model(path, quant_config=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",  # Auto-shard if needed\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()  # Eval mode\n",
    "    return model\n",
    "# Function to generate with chat template\n",
    "def generate_response(generator, prompt, reference):\n",
    "    # Structure as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt.split(\"Player:\")[0].strip()},  # E.g., \"You are a grumpy blacksmith.\"\n",
    "        {\"role\": \"user\", \"content\": prompt.split(\"Player:\")[1].strip() if \"Player:\" in prompt else prompt}  # E.g., \"What about the dragon?\"\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return generator(formatted_prompt, return_full_text=False)[0][\"generated_text\"]  # Don't echo prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c0339cf-703c-496c-9fe5-edec0cf56b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a02c8430204c19a0d8f14721ee0feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9a4dc73f7448948e81ea9c5ed343a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quantization config (4-bit to fit on GPU)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "fine_model = load_model(\"./npc_finetuned_bertscore-eval-noeval\", quant_config)\n",
    "fine_generator = pipeline(\"text-generation\", model=fine_model, tokenizer=tokenizer, max_new_tokens=max_token, device_map=\"auto\")\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "base_model = load_model(\"microsoft/Phi-3-mini-4k-instruct\", quant_config)  # Assuming this is your model load function\n",
    "base_model.to(device)  # Ensure on device\n",
    "base_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=max_token, device_map=\"auto\")\n",
    "\n",
    "\n",
    "# gen_replies = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     item = format_eval_example(row)\n",
    "#     res = generate_response(fine_generator, item[\"prompt\"], item[\"reference\"]).strip()\n",
    "#     gen_replies.append(res)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cb2806d-b1a3-4e74-a5bf-2d197b6bcf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8049d5b-b3c6-4ab7-86b6-a2b2231948c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [10:03<00:00, 100.54s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define collate for batching\n",
    "def collate_fn(batch):\n",
    "    prompts = [format_eval_example(row)[\"prompt\"] for row in batch]  # Extract prompts\n",
    "    references = [format_eval_example(row)[\"reference\"] for row in batch]  # Extract references if needed\n",
    "    return prompts, references\n",
    "\n",
    "dataloader = DataLoader(df.to_dict('records'), batch_size=32, collate_fn=collate_fn)  # Batch size adjust based on VRAM\n",
    "\n",
    "based_gen_replies = []\n",
    "gen_replies = []\n",
    "for batch_prompts, batch_refs in tqdm(dataloader):\n",
    "    # Tokenize batched prompts\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Generate in batch\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(**inputs, max_new_tokens=max_token, do_sample=False)  # Adjust sampling if needed\n",
    "        outputs_ft = fine_model.generate(**inputs, max_new_tokens=max_token, do_sample=False)  # Adjust sampling if needed\n",
    "    \n",
    "    # Decode\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_ft = tokenizer.batch_decode(outputs_ft, skip_special_tokens=True)\n",
    "    \n",
    "    # Process each (strip, etc.)\n",
    "    res_batch = [generate_response(base_generator, prompt, ref).strip() for prompt, ref, dec in zip(batch_prompts, batch_refs, decoded)]\n",
    "    res_batch = [x.strip('\"') for x in res_batch]\n",
    "    \n",
    "    res_batch_ft = [generate_response(fine_generator, prompt, ref).strip() for prompt, ref, dec in zip(batch_prompts, batch_refs, decoded_ft)]\n",
    "    res_batch_ft = [x.strip('\"') for x in res_batch_ft]\n",
    "    \n",
    "    based_gen_replies.extend(res_batch)\n",
    "    gen_replies.extend(res_batch_ft)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "249012ac-3614-410b-9e21-f81c32002c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gen_replies'] = gen_replies\n",
    "df['based_replies'] = based_gen_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ea5c0c7-77c5-40d2-a57c-89a13c006f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>gen_replies</th>\n",
       "      <th>based_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensuring every student receives the individual...</td>\n",
       "      <td>Lack of resources and support.</td>\n",
       "      <td>The biggest challenge I face as a teacher is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's just who I am, I guess. I love seeing peo...</td>\n",
       "      <td>I find it amusing to watch people's reactions ...</td>\n",
       "      <td>Ah, my dear friend, your curiosity warms my he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Courageous, dedicated, honorable.\"</td>\n",
       "      <td>Honorable, fearless, dedicated.</td>\n",
       "      <td>As a Knight Templar, I can be described as val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cities are noisy and overwhelming.</td>\n",
       "      <td>Cities are no place for a ranger.</td>\n",
       "      <td>No, I have never been to a city. As a creature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My country and the people I love.</td>\n",
       "      <td>My family, they are my motivation.</td>\n",
       "      <td>As Tiger, my work and my team are the most val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Yes, I have a magical tome that has been passe...</td>\n",
       "      <td>I have a magical amulet that has been passed d...</td>\n",
       "      <td>Ah, the question of magic and its cherished tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>To see the natural world flourish, long after ...</td>\n",
       "      <td>That is difficult to answer. I live in the mom...</td>\n",
       "      <td>Ah, my dear adventurers, I often ponder the gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Against Queen Nehelenia, she was a tough oppon...</td>\n",
       "      <td>The battle against Queen Beryl, the sorceress,...</td>\n",
       "      <td>As a Sailor Scout, I've faced countless battle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>\"Difficult decisions, for the greater good.\"</td>\n",
       "      <td>Difficult, but necessary.</td>\n",
       "      <td>Indeed, as a Knight Templar, I have faced many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>They are a nuisance that must be dealt with.</td>\n",
       "      <td>They're just obstacles in my path.</td>\n",
       "      <td>Well, they do try, don't they? Always chasing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Response  \\\n",
       "0    Ensuring every student receives the individual...   \n",
       "1    It's just who I am, I guess. I love seeing peo...   \n",
       "2                  \"Courageous, dedicated, honorable.\"   \n",
       "3                   Cities are noisy and overwhelming.   \n",
       "4                    My country and the people I love.   \n",
       "..                                                 ...   \n",
       "187  Yes, I have a magical tome that has been passe...   \n",
       "188  To see the natural world flourish, long after ...   \n",
       "189  Against Queen Nehelenia, she was a tough oppon...   \n",
       "190       \"Difficult decisions, for the greater good.\"   \n",
       "191       They are a nuisance that must be dealt with.   \n",
       "\n",
       "                                           gen_replies  \\\n",
       "0                       Lack of resources and support.   \n",
       "1    I find it amusing to watch people's reactions ...   \n",
       "2                      Honorable, fearless, dedicated.   \n",
       "3                    Cities are no place for a ranger.   \n",
       "4                   My family, they are my motivation.   \n",
       "..                                                 ...   \n",
       "187  I have a magical amulet that has been passed d...   \n",
       "188  That is difficult to answer. I live in the mom...   \n",
       "189  The battle against Queen Beryl, the sorceress,...   \n",
       "190                          Difficult, but necessary.   \n",
       "191                 They're just obstacles in my path.   \n",
       "\n",
       "                                         based_replies  \n",
       "0    The biggest challenge I face as a teacher is b...  \n",
       "1    Ah, my dear friend, your curiosity warms my he...  \n",
       "2    As a Knight Templar, I can be described as val...  \n",
       "3    No, I have never been to a city. As a creature...  \n",
       "4    As Tiger, my work and my team are the most val...  \n",
       "..                                                 ...  \n",
       "187  Ah, the question of magic and its cherished tr...  \n",
       "188  Ah, my dear adventurers, I often ponder the gr...  \n",
       "189  As a Sailor Scout, I've faced countless battle...  \n",
       "190  Indeed, as a Knight Templar, I have faced many...  \n",
       "191  Well, they do try, don't they? Always chasing ...  \n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Response', 'gen_replies', 'based_replies']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb03843-439a-4740-8788-957d85b63ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>gen_replies</th>\n",
       "      <th>based_replies</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naina Mathur</td>\n",
       "      <td>Naina Mathur is a determined and passionate te...</td>\n",
       "      <td>What is the biggest challenge you face as a te...</td>\n",
       "      <td>Ensuring every student receives the individual...</td>\n",
       "      <td>Concern</td>\n",
       "      <td>Lack of resources and support.</td>\n",
       "      <td>The biggest challenge I face as a teacher is b...</td>\n",
       "      <td>You are Naina Mathur, Naina Mathur is a determ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zephyr</td>\n",
       "      <td>Zephyr is a mischievous fairy who loves playin...</td>\n",
       "      <td>What motivates you to play pranks on people?</td>\n",
       "      <td>It's just who I am, I guess. I love seeing peo...</td>\n",
       "      <td>Playfulness</td>\n",
       "      <td>I find it amusing to watch people's reactions ...</td>\n",
       "      <td>Ah, my dear friend, your curiosity warms my he...</td>\n",
       "      <td>You are Zephyr, Zephyr is a mischievous fairy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arn, the Knight Templar</td>\n",
       "      <td>Arn is a highly skilled and honorable knight,</td>\n",
       "      <td>Can you describe yourself in three words?</td>\n",
       "      <td>\"Courageous, dedicated, honorable.\"</td>\n",
       "      <td>Pride</td>\n",
       "      <td>Honorable, fearless, dedicated.</td>\n",
       "      <td>As a Knight Templar, I can be described as val...</td>\n",
       "      <td>You are Arn, the Knight Templar, Arn is a high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arinthal</td>\n",
       "      <td>Arinthal is an elven ranger from the ancient f...</td>\n",
       "      <td>Have you ever been to a city?</td>\n",
       "      <td>Cities are noisy and overwhelming.</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>Cities are no place for a ranger.</td>\n",
       "      <td>No, I have never been to a city. As a creature...</td>\n",
       "      <td>You are Arinthal, Arinthal is an elven ranger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tiger</td>\n",
       "      <td>Tiger is a highly skilled and fearless spy wor...</td>\n",
       "      <td>What is the most valuable thing in your life?</td>\n",
       "      <td>My country and the people I love.</td>\n",
       "      <td>Love</td>\n",
       "      <td>My family, they are my motivation.</td>\n",
       "      <td>As Tiger, my work and my team are the most val...</td>\n",
       "      <td>You are Tiger, Tiger is a highly skilled and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Marcella Ravenwood</td>\n",
       "      <td>Marcella Ravenwood is a powerful sorceress who...</td>\n",
       "      <td>Do you have any magical artifacts that you che...</td>\n",
       "      <td>Yes, I have a magical tome that has been passe...</td>\n",
       "      <td>Sentimental</td>\n",
       "      <td>I have a magical amulet that has been passed d...</td>\n",
       "      <td>Ah, the question of magic and its cherished tr...</td>\n",
       "      <td>You are Marcella Ravenwood, Marcella Ravenwood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Lyra Dawnstrider</td>\n",
       "      <td>Lyra Dawnstrider is a high-elf ranger from the...</td>\n",
       "      <td>What is your ultimate goal in life?</td>\n",
       "      <td>To see the natural world flourish, long after ...</td>\n",
       "      <td>Peacefulness</td>\n",
       "      <td>That is difficult to answer. I live in the mom...</td>\n",
       "      <td>Ah, my dear adventurers, I often ponder the gr...</td>\n",
       "      <td>You are Lyra Dawnstrider, Lyra Dawnstrider is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Sailor Moon</td>\n",
       "      <td>Sailor Moon is the protector of the galaxy, de...</td>\n",
       "      <td>What is the most challenging battle you've fou...</td>\n",
       "      <td>Against Queen Nehelenia, she was a tough oppon...</td>\n",
       "      <td>Triumphant</td>\n",
       "      <td>The battle against Queen Beryl, the sorceress,...</td>\n",
       "      <td>As a Sailor Scout, I've faced countless battle...</td>\n",
       "      <td>You are Sailor Moon, Sailor Moon is the protec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Arn, the Knight Templar</td>\n",
       "      <td>Arn is a highly skilled and honorable knight,</td>\n",
       "      <td>Have you ever made a difficult decision?</td>\n",
       "      <td>\"Difficult decisions, for the greater good.\"</td>\n",
       "      <td>Conviction</td>\n",
       "      <td>Difficult, but necessary.</td>\n",
       "      <td>Indeed, as a Knight Templar, I have faced many...</td>\n",
       "      <td>You are Arn, the Knight Templar, Arn is a high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Light Yagami</td>\n",
       "      <td>Light Yagami is a highly intelligent high scho...</td>\n",
       "      <td>What do you think of the police?</td>\n",
       "      <td>They are a nuisance that must be dealt with.</td>\n",
       "      <td>Contemptuous</td>\n",
       "      <td>They're just obstacles in my path.</td>\n",
       "      <td>Well, they do try, don't they? Always chasing ...</td>\n",
       "      <td>You are Light Yagami, Light Yagami is a highly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  \\\n",
       "0               Naina Mathur   \n",
       "1                     Zephyr   \n",
       "2    Arn, the Knight Templar   \n",
       "3                   Arinthal   \n",
       "4                      Tiger   \n",
       "..                       ...   \n",
       "187       Marcella Ravenwood   \n",
       "188         Lyra Dawnstrider   \n",
       "189              Sailor Moon   \n",
       "190  Arn, the Knight Templar   \n",
       "191             Light Yagami   \n",
       "\n",
       "                                             Biography  \\\n",
       "0    Naina Mathur is a determined and passionate te...   \n",
       "1    Zephyr is a mischievous fairy who loves playin...   \n",
       "2        Arn is a highly skilled and honorable knight,   \n",
       "3    Arinthal is an elven ranger from the ancient f...   \n",
       "4    Tiger is a highly skilled and fearless spy wor...   \n",
       "..                                                 ...   \n",
       "187  Marcella Ravenwood is a powerful sorceress who...   \n",
       "188  Lyra Dawnstrider is a high-elf ranger from the...   \n",
       "189  Sailor Moon is the protector of the galaxy, de...   \n",
       "190      Arn is a highly skilled and honorable knight,   \n",
       "191  Light Yagami is a highly intelligent high scho...   \n",
       "\n",
       "                                                 Query  \\\n",
       "0    What is the biggest challenge you face as a te...   \n",
       "1         What motivates you to play pranks on people?   \n",
       "2            Can you describe yourself in three words?   \n",
       "3                        Have you ever been to a city?   \n",
       "4        What is the most valuable thing in your life?   \n",
       "..                                                 ...   \n",
       "187  Do you have any magical artifacts that you che...   \n",
       "188                What is your ultimate goal in life?   \n",
       "189  What is the most challenging battle you've fou...   \n",
       "190           Have you ever made a difficult decision?   \n",
       "191                   What do you think of the police?   \n",
       "\n",
       "                                              Response       Emotion  \\\n",
       "0    Ensuring every student receives the individual...       Concern   \n",
       "1    It's just who I am, I guess. I love seeing peo...   Playfulness   \n",
       "2                  \"Courageous, dedicated, honorable.\"         Pride   \n",
       "3                   Cities are noisy and overwhelming.       Disgust   \n",
       "4                    My country and the people I love.          Love   \n",
       "..                                                 ...           ...   \n",
       "187  Yes, I have a magical tome that has been passe...   Sentimental   \n",
       "188  To see the natural world flourish, long after ...  Peacefulness   \n",
       "189  Against Queen Nehelenia, she was a tough oppon...    Triumphant   \n",
       "190       \"Difficult decisions, for the greater good.\"    Conviction   \n",
       "191       They are a nuisance that must be dealt with.  Contemptuous   \n",
       "\n",
       "                                           gen_replies  \\\n",
       "0                       Lack of resources and support.   \n",
       "1    I find it amusing to watch people's reactions ...   \n",
       "2                      Honorable, fearless, dedicated.   \n",
       "3                    Cities are no place for a ranger.   \n",
       "4                   My family, they are my motivation.   \n",
       "..                                                 ...   \n",
       "187  I have a magical amulet that has been passed d...   \n",
       "188  That is difficult to answer. I live in the mom...   \n",
       "189  The battle against Queen Beryl, the sorceress,...   \n",
       "190                          Difficult, but necessary.   \n",
       "191                 They're just obstacles in my path.   \n",
       "\n",
       "                                         based_replies  \\\n",
       "0    The biggest challenge I face as a teacher is b...   \n",
       "1    Ah, my dear friend, your curiosity warms my he...   \n",
       "2    As a Knight Templar, I can be described as val...   \n",
       "3    No, I have never been to a city. As a creature...   \n",
       "4    As Tiger, my work and my team are the most val...   \n",
       "..                                                 ...   \n",
       "187  Ah, the question of magic and its cherished tr...   \n",
       "188  Ah, my dear adventurers, I often ponder the gr...   \n",
       "189  As a Sailor Scout, I've faced countless battle...   \n",
       "190  Indeed, as a Knight Templar, I have faced many...   \n",
       "191  Well, they do try, don't they? Always chasing ...   \n",
       "\n",
       "                                                prompt  \n",
       "0    You are Naina Mathur, Naina Mathur is a determ...  \n",
       "1    You are Zephyr, Zephyr is a mischievous fairy ...  \n",
       "2    You are Arn, the Knight Templar, Arn is a high...  \n",
       "3    You are Arinthal, Arinthal is an elven ranger ...  \n",
       "4    You are Tiger, Tiger is a highly skilled and f...  \n",
       "..                                                 ...  \n",
       "187  You are Marcella Ravenwood, Marcella Ravenwood...  \n",
       "188  You are Lyra Dawnstrider, Lyra Dawnstrider is ...  \n",
       "189  You are Sailor Moon, Sailor Moon is the protec...  \n",
       "190  You are Arn, the Knight Templar, Arn is a high...  \n",
       "191  You are Light Yagami, Light Yagami is a highly...  \n",
       "\n",
       "[192 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31fb939a-6506-4e41-8570-22bbd368dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(example):\n",
    "    system = f\"You are {example['Name']}, {example['Biography']}. Respond in character with emotion: {example['Emotion']}.\"\n",
    "    return system\n",
    "    \n",
    "prompts_lst = []\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = get_prompt(row)\n",
    "    prompts_lst.append(prompt)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c98da30f-0cd5-4dbf-afca-64ccf52fb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = prompts_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbb150bb-5e33-488e-9609-8ad6d6b84fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('final_res_test_max_token150.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c36e91-3198-4648-9573-2cca40096709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
